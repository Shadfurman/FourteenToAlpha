{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadfurman/FourteenToAlpha/blob/main/trading_bot_scaffold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wILlIqQMcY2"
      },
      "source": [
        "# Trading Bot: Conv-Transformer → Backtest (Scaffold)\n",
        "\n",
        "End-to-end notebook scaffold. Fill in the TODOs only—no scope creep.\n",
        "\n",
        "**Sections:**\n",
        "- Setup & seed\n",
        "- Data & features\n",
        "- Split\n",
        "- Model\n",
        "- Train\n",
        "- Inference & backtest\n",
        "- Metrics & plots\n",
        "- Params dump & outputs\n"
      ],
      "id": "5wILlIqQMcY2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsV6n4toMcY8"
      },
      "source": [
        "## Setup & seed"
      ],
      "id": "gsV6n4toMcY8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Uihs1lMcY-",
        "outputId": "4f54d3da-0f20-48ab-9d65-57c97e946158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda [Tesla T4]\n",
            "Seed & PARAMS set; outputs/params_seed.json written.\n"
          ]
        }
      ],
      "source": [
        "# Minimal installs for Colab (comment any you don't need)\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip -q install yfinance pandas numpy scikit-learn matplotlib torch torchvision torchaudio pyarrow --progress-bar off\n",
        "\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "SEED = 42  # <-- SINGLE SOURCE OF TRUTH\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)\n",
        "\n",
        "# Global paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "PARQUET_DIR = '/content/drive/MyDrive/stock_market_data/data_parquet'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# PARAMS — edit here only\n",
        "PARAMS = {\n",
        "    'tickers': \"ALL_PARQUET\",  # will discover from parquet folder\n",
        "    'start': '1970-01-01',\n",
        "    'end': '2025-01-01',\n",
        "    'lookback': 64,\n",
        "    'features': ['pct_change', 'roll_mean_10', 'roll_vol_10', 'rsi_14'],\n",
        "    'split': {\n",
        "        'train_start': '2010-01-01', 'train_end': '2018-12-31',\n",
        "        'val_start':   '2019-01-01', 'val_end':   '2019-12-31',\n",
        "        'test_start':  '2020-01-01', 'test_end':  '2024-12-31',\n",
        "    },\n",
        "    'model_option': 'A',\n",
        "    'epochs': 12,\n",
        "    'batch_size': 256,\n",
        "    'lr': 1e-3,\n",
        "    'threshold_long': 0.55,\n",
        "    'threshold_short': 0.45,\n",
        "    'cost_bps': 5,\n",
        "}\n",
        "with open(os.path.join(OUTPUT_DIR, 'params_seed.json'), 'w') as f:\n",
        "    json.dump({'seed': SEED, 'params': PARAMS}, f, indent=2)\n",
        "\n",
        "# Quick environment check\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "gpu = torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\"\n",
        "print(f\"Device: {device} [{gpu}]\")\n",
        "print('Seed & PARAMS set; outputs/params_seed.json written.')\n"
      ],
      "id": "H9Uihs1lMcY-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYs4IQaU0ZEL",
        "outputId": "128193e3-b035-4351-b2f1-c4d95fb2d966"
      },
      "id": "eYs4IQaU0ZEL",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPueuQsHMcZA"
      },
      "source": [
        "## Data & features"
      ],
      "id": "uPueuQsHMcZA"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8hKKGhFMcZB",
        "outputId": "4c65d456-2d47-4648-ec40-4433f6ae17af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data/feature functions defined. Fill TODOs as needed.\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def load_ohlcv_yf(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"Load daily OHLCV from yfinance. TODO: cache to CSV in ./data.\"\"\"\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(f'No data for {ticker}')\n",
        "    df = df[['Open','High','Low','Close','Volume']].copy()\n",
        "    df.index.name = 'Date'\n",
        "    df['Ticker'] = ticker\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
        "    series = series.astype(float)\n",
        "    delta = series.diff()\n",
        "    up = delta.clip(lower=0.0)\n",
        "    down = (-delta).clip(lower=0.0)\n",
        "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
        "    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
        "    rs = roll_up / (roll_down + 1e-8)\n",
        "    return 100.0 - (100.0 / (1.0 + rs))\n",
        "\n",
        "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute pct-change, rolling mean/vol(10), RSI(14) on adjusted close if present.\n",
        "    Add next-day direction target y (1 if up, else 0). Drops NaNs and sorts index.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    out = out.sort_index()\n",
        "\n",
        "    # Prefer adjusted. If converter already replaced Close with adjusted, this is a no-op.\n",
        "    price = out['AdjClose'] if 'AdjClose' in out.columns else out['Close']\n",
        "\n",
        "    out['pct_change']   = price.pct_change()\n",
        "    out['roll_mean_10'] = out['pct_change'].rolling(window=10, min_periods=10).mean()\n",
        "    out['roll_vol_10']  = out['pct_change'].rolling(window=10, min_periods=10).std()\n",
        "    out['rsi_14']       = compute_rsi(price, 14)\n",
        "\n",
        "    # Target uses forward return on the same price series\n",
        "    out['ret_fwd_1'] = price.pct_change().shift(-1)\n",
        "    out['y'] = (out['ret_fwd_1'] > 0).astype(int)\n",
        "\n",
        "    # Final cleanup: drop rows with any NaNs in features/target\n",
        "    out = out.dropna(subset=['pct_change', 'roll_mean_10', 'roll_vol_10', 'rsi_14', 'ret_fwd_1', 'y'])\n",
        "    return out\n",
        "\n",
        "def load_all_tickers(tickers, start, end):\n",
        "    frames = []\n",
        "    for t in tickers:\n",
        "        raw = load_ohlcv_yf(t, start, end)\n",
        "        feats = make_features(raw)\n",
        "        frames.append(feats)\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "# TODO: Optionally visualize a sample\n",
        "print('Data/feature functions defined. Fill TODOs as needed.')\n"
      ],
      "id": "N8hKKGhFMcZB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV→Parquet converter"
      ],
      "metadata": {
        "id": "RwhtxRzKprt3"
      },
      "id": "RwhtxRzKprt3"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSV → Parquet (one-time) ---\n",
        "import glob\n",
        "\n",
        "# Your paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "#PARQUET_DIR = '/content/data_parquet'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# Map your CSV headers to canonical names we use elsewhere\n",
        "CSV_COLS = {\n",
        "    'Date': 'Date',\n",
        "    'Open': 'Open',\n",
        "    'High': 'High',\n",
        "    'Low': 'Low',\n",
        "    'Close': 'Close',                 # raw close (we'll replace with Adjusted Close for features)\n",
        "    'Volume': 'Volume',\n",
        "    'Adjusted Close': 'AdjClose',\n",
        "}\n",
        "\n",
        "def guess_ticker_from_path(p):\n",
        "    import os\n",
        "    return os.path.splitext(os.path.basename(p))[0].upper()\n",
        "\n",
        "def load_csv_one(p):\n",
        "    # Read permissively (handles odd rows/column orders)\n",
        "    df = pd.read_csv(\n",
        "        p,\n",
        "        engine='python',           # robust parser\n",
        "        on_bad_lines='skip'        # skip corrupted rows\n",
        "    )\n",
        "\n",
        "    # Normalize column names (lower -> canonical)\n",
        "    orig_cols = df.columns.tolist()\n",
        "    lc_map = {c.lower().strip(): c for c in df.columns}\n",
        "\n",
        "    # Find variants\n",
        "    date_col = lc_map.get('date')\n",
        "    open_col = lc_map.get('open')\n",
        "    high_col = lc_map.get('high')\n",
        "    low_col  = lc_map.get('low')\n",
        "    close_col= lc_map.get('close')\n",
        "    vol_col  = lc_map.get('volume')\n",
        "    adj_col  = lc_map.get('adjusted close') or lc_map.get('adj close') or lc_map.get('adjclose')\n",
        "\n",
        "    # Hard fail if no Date\n",
        "    if not date_col:\n",
        "        raise ValueError(f'No Date column in {p} (had: {orig_cols})')\n",
        "\n",
        "    # Build a minimal frame with whatever we found\n",
        "    cols = {}\n",
        "    cols['Date']   = date_col\n",
        "    if open_col:  cols['Open']  = open_col\n",
        "    if high_col:  cols['High']  = high_col\n",
        "    if low_col:   cols['Low']   = low_col\n",
        "    if close_col: cols['Close'] = close_col\n",
        "    if vol_col:   cols['Volume']= vol_col\n",
        "    if adj_col:   cols['AdjClose'] = adj_col\n",
        "\n",
        "    df = df.rename(columns={v:k for k,v in cols.items()})[list(cols.keys())]\n",
        "\n",
        "    # Parse day-first dates like 15-12-2010\n",
        "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "    df = df.dropna(subset=['Date']).sort_values('Date').set_index('Date')\n",
        "\n",
        "    # Prefer adjusted close for features; fall back to Close\n",
        "    if 'AdjClose' in df.columns:\n",
        "        df['Close_raw'] = df.get('Close', df['AdjClose'])\n",
        "        df['Close'] = df['AdjClose']\n",
        "    else:\n",
        "        # No adjusted close available; use raw Close\n",
        "        if 'Close' not in df.columns:\n",
        "            raise ValueError(f'No Close/AdjClose in {p} (had: {orig_cols})')\n",
        "        df['Close_raw'] = df['Close']\n",
        "\n",
        "    # Coerce numerics (strip stray chars like $ or commas)\n",
        "    for c in ['Open','High','Low','Close','Volume','AdjClose','Close_raw']:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.replace(r'[^0-9\\.\\-eE]', '', regex=True)\n",
        "                           .replace({'': None}))\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "    # Basic sanity: drop rows missing Close or Volume\n",
        "    need = ['Close']\n",
        "    if 'Volume' in df.columns: need.append('Volume')\n",
        "    df = df.dropna(subset=need)\n",
        "\n",
        "    df['Ticker'] = guess_ticker_from_path(p)\n",
        "    return df\n",
        "\n",
        "def iter_csv_files():\n",
        "    for d in DATA_IN_DIRS:\n",
        "        for p in glob.glob(os.path.join(d, '*.csv')):\n",
        "            yield p\n",
        "\n",
        "def build_parquet_once():\n",
        "    converted, skipped = 0, []\n",
        "    for p in iter_csv_files():\n",
        "        tkr = guess_ticker_from_path(p)\n",
        "        outp = os.path.join(PARQUET_DIR, f'{tkr}.parquet')\n",
        "        if os.path.exists(outp):\n",
        "            continue\n",
        "        try:\n",
        "            df = load_csv_one(p)\n",
        "            feats = make_features(df)  # uses adjusted Close internally now\n",
        "            feats = feats.drop(columns=[c for c in ['AdjClose','Close_raw'] if c in feats.columns], errors='ignore')\n",
        "            feats.to_parquet(outp)\n",
        "            converted += 1\n",
        "            if converted % 50 == 0:\n",
        "                print(f'Converted {converted} tickers...')\n",
        "        except Exception as e:\n",
        "            skipped.append((tkr, str(e)))\n",
        "            if len(skipped) <= 5:\n",
        "                print(f'[WARN] Skipping {tkr}: {e}')\n",
        "    print(f'Parquet conversion complete. Total tickers converted: {converted}. Skipped: {len(skipped)}')\n",
        "    if skipped:\n",
        "        import json\n",
        "        with open(os.path.join(OUTPUT_DIR, 'skipped_files.json'), 'w') as f:\n",
        "            json.dump(skipped, f, indent=2)\n",
        "        print('Wrote outputs/skipped_files.json')\n",
        "\n",
        "build_parquet_once()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "iBt_58EIqn1U",
        "outputId": "67103294-a30f-4235-f55b-f815b41c8cb8"
      },
      "id": "iBt_58EIqn1U",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-748173821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wrote outputs/skipped_files.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mbuild_parquet_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-748173821.py\u001b[0m in \u001b[0;36mbuild_parquet_once\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_parquet_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mconverted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter_csv_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mtkr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mguess_ticker_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0moutp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARQUET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{tkr}.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-748173821.py\u001b[0m in \u001b[0;36miter_csv_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0miter_csv_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATA_IN_DIRS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36mglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, include_hidden)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mzero\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mdirectories\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msubdirectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m     return list(iglob(pathname, root_dir=root_dir, dir_fd=dir_fd, recursive=recursive,\n\u001b[0m\u001b[1;32m     29\u001b[0m                       include_hidden=include_hidden))\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mglob_in_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_glob0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         for name in glob_in_dir(_join(root_dir, dirname), basename, dir_fd, dironly,\n\u001b[0m\u001b[1;32m     98\u001b[0m                                include_hidden=include_hidden):\n\u001b[1;32m     99\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dir_fd, dironly, include_hidden)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_glob1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_listdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_hidden\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ishidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minclude_hidden\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ishidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36m_listdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_listdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdironly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m# Recursively yields relative pathnames inside a literal directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/glob.py\u001b[0m in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0marg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parquet Loader"
      ],
      "metadata": {
        "id": "6lra6mHfrkVu"
      },
      "id": "6lra6mHfrkVu"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parquet path index (no big concat) ---\n",
        "import glob, os\n",
        "from datetime import datetime\n",
        "\n",
        "#PARQUET_DIR = '/content/data_parquet'  # already set\n",
        "paths = sorted(glob.glob(os.path.join(PARQUET_DIR, '*.parquet')))\n",
        "\n",
        "# Utility to count rows in a date window for one ticker without keeping it in RAM\n",
        "def count_rows_in_range(p, start, end):\n",
        "    df = pd.read_parquet(p, columns=['Ticker'])  # small read\n",
        "    # quick read of index range by reloading with date filter\n",
        "    df = pd.read_parquet(p)  # still per-ticker, small-ish\n",
        "    df = df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    return len(df)\n",
        "\n",
        "split_cfg = PARAMS['split']\n",
        "# Build file lists for each split (only tickers with enough rows in ALL splits)\n",
        "keep_paths = []\n",
        "train_paths, val_paths, test_paths = [], [], []\n",
        "\n",
        "lookback = PARAMS['lookback']\n",
        "for p in paths:\n",
        "    n_train = count_rows_in_range(p, split_cfg['train_start'], split_cfg['train_end'])\n",
        "    n_val   = count_rows_in_range(p, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "    n_test  = count_rows_in_range(p, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "    if (n_train >= lookback + 2) and (n_val >= lookback + 2) and (n_test >= lookback + 2):\n",
        "        keep_paths.append(p)\n",
        "\n",
        "def filter_paths_by_range(paths, start, end):\n",
        "    out = []\n",
        "    for p in paths:\n",
        "        # quick check again to avoid reusing tiny tickers\n",
        "        n = count_rows_in_range(p, start, end)\n",
        "        if n >= lookback + 2:\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "train_paths = filter_paths_by_range(keep_paths, split_cfg['train_start'], split_cfg['train_end'])\n",
        "val_paths   = filter_paths_by_range(keep_paths, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "test_paths  = filter_paths_by_range(keep_paths, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "\n",
        "print(f\"tickers kept: {len(keep_paths)}\")\n",
        "print(f\"train_paths: {len(train_paths)}, val_paths: {len(val_paths)}, test_paths: {len(test_paths)}\")"
      ],
      "metadata": {
        "id": "Mv0EewBFrq04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbacf88-0976-47f5-927f-a72f91656300"
      },
      "id": "Mv0EewBFrq04",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tickers kept: 2406\n",
            "train_paths: 2406, val_paths: 2406, test_paths: 2406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build full_df for split"
      ],
      "metadata": {
        "id": "H3DBaQOssy_w"
      },
      "id": "H3DBaQOssy_w"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build full_df for split (features + target only) ---\n",
        "import glob, os\n",
        "\n",
        "MIN_COLS = PARAMS['features'] + ['y', 'Ticker']\n",
        "\n",
        "def load_all_parquet_min(parquet_dir=PARQUET_DIR, columns=MIN_COLS):\n",
        "    frames = []\n",
        "    for p in glob.glob(os.path.join(parquet_dir, '*.parquet')):\n",
        "        frames.append(pd.read_parquet(p, columns=columns))\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "full_df = load_all_parquet_min()\n",
        "print('full_df:', len(full_df), 'rows |', full_df['Ticker'].nunique(), 'tickers')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQvgb7Pys4ej",
        "outputId": "81ac63ba-27af-4a08-a297-df54167e1a3e"
      },
      "id": "DQvgb7Pys4ej",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full_df: 15236147 rows | 2660 tickers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgWbAE8HMcZC"
      },
      "source": [
        "## Split (rolling walk-forward)"
      ],
      "id": "LgWbAE8HMcZC"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-tFob2ZyMcZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e88a54-32a5-4aec-8975-6814abd8864b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  2406 tickers |   5036427 rows\n",
            "val:  2406 tickers |    606112 rows\n",
            "test:  2406 tickers |   1778231 rows\n"
          ]
        }
      ],
      "source": [
        "# --- Walk-forward split + filter short tickers ---\n",
        "# assumes: full_df, PARAMS, get_splits() already defined\n",
        "\n",
        "def time_slice(df, start, end):\n",
        "    return df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "\n",
        "def get_splits(df, split_cfg):\n",
        "    train_df = time_slice(df, split_cfg['train_start'], split_cfg['train_end'])\n",
        "    val_df   = time_slice(df, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "    test_df  = time_slice(df, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "train_df, val_df, test_df = get_splits(full_df, PARAMS['split'])\n",
        "\n",
        "def min_len_ok(df, lookback):\n",
        "    # need at least lookback + 2 rows to form a label and 1-step shift\n",
        "    return df.groupby('Ticker').size() >= (lookback + 2)\n",
        "\n",
        "lookback = PARAMS['lookback']\n",
        "\n",
        "ok_train = set(min_len_ok(train_df, lookback).pipe(lambda s: s[s].index))\n",
        "ok_val   = set(min_len_ok(val_df,   lookback).pipe(lambda s: s[s].index))\n",
        "ok_test  = set(min_len_ok(test_df,  lookback).pipe(lambda s: s[s].index))\n",
        "\n",
        "# keep tickers present (and long enough) in ALL splits to avoid leakage/imbalance\n",
        "keep = ok_train & ok_val & ok_test\n",
        "\n",
        "def keep_tickers(df, tickers):\n",
        "    return df[df['Ticker'].isin(tickers)].copy()\n",
        "\n",
        "train_df = keep_tickers(train_df, keep)\n",
        "val_df   = keep_tickers(val_df,   keep)\n",
        "test_df  = keep_tickers(test_df,  keep)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"{name}: {df['Ticker'].nunique():>5} tickers | {len(df):>9} rows\")\n"
      ],
      "id": "-tFob2ZyMcZC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Walk-forward split + filter short tickers + build SeqDatasets\n"
      ],
      "metadata": {
        "id": "Zx-pG2pbOQYK"
      },
      "id": "Zx-pG2pbOQYK"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Walk-forward split + filter short tickers + build SeqDatasets (self-contained) ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Define get_splits if not already defined\n",
        "if 'get_splits' not in globals():\n",
        "    def time_slice(df, start, end):\n",
        "        return df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    def get_splits(df, split_cfg):\n",
        "        train_df = time_slice(df, split_cfg['train_start'], split_cfg['train_end'])\n",
        "        val_df   = time_slice(df, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "        test_df  = time_slice(df, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "train_df, val_df, test_df = get_splits(full_df, PARAMS['split'])\n",
        "\n",
        "# 2) Keep only tickers with enough rows in ALL splits (avoid short series)\n",
        "def min_len_ok(df, lookback):\n",
        "    return df.groupby('Ticker').size() >= (lookback + 2)\n",
        "\n",
        "lookback = PARAMS['lookback']\n",
        "ok_train = set(min_len_ok(train_df, lookback).pipe(lambda s: s[s].index))\n",
        "ok_val   = set(min_len_ok(val_df,   lookback).pipe(lambda s: s[s].index))\n",
        "ok_test  = set(min_len_ok(test_df,  lookback).pipe(lambda s: s[s].index))\n",
        "keep = ok_train & ok_val & ok_test\n",
        "\n",
        "def keep_tickers(df, tickers):\n",
        "    return df[df['Ticker'].isin(tickers)].copy()\n",
        "\n",
        "train_df = keep_tickers(train_df, keep)\n",
        "val_df   = keep_tickers(val_df,   keep)\n",
        "test_df  = keep_tickers(test_df,  keep)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"{name}: {df['Ticker'].nunique():>5} tickers | {len(df):>9} rows\")\n",
        "\n",
        "# --- Fix boundary leakage: drop last row per ticker within each split ---\n",
        "def drop_last_per_ticker(df):\n",
        "    df = df.sort_index()\n",
        "    last_idx = df.groupby('Ticker').tail(1).index\n",
        "    return df.drop(index=last_idx)\n",
        "\n",
        "train_df = drop_last_per_ticker(train_df)\n",
        "val_df   = drop_last_per_ticker(val_df)\n",
        "test_df  = drop_last_per_ticker(test_df)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"{name}: {df['Ticker'].nunique():>5} tickers | {len(df):>9} rows (boundary-safe)\")\n",
        "\n",
        "# --- Normalize features using train stats only ---\n",
        "feat_cols = PARAMS['features']\n",
        "\n",
        "# compute stats on train only\n",
        "feat_means = train_df[feat_cols].mean()\n",
        "feat_stds  = train_df[feat_cols].std().replace(0, 1.0)  # avoid div-by-zero\n",
        "\n",
        "def apply_norm(df):\n",
        "    df = df.copy()\n",
        "    df[feat_cols] = (df[feat_cols] - feat_means) / feat_stds\n",
        "    return df\n",
        "\n",
        "train_df = apply_norm(train_df)\n",
        "val_df   = apply_norm(val_df)\n",
        "test_df  = apply_norm(test_df)\n",
        "\n",
        "# quick sanity\n",
        "print(\"Post-normalization sanity (should be ~0/1 on train):\")\n",
        "print(\" train means:\\n\", train_df[feat_cols].mean().round(3))\n",
        "print(\" train stds:\\n\",  train_df[feat_cols].std().round(3))\n",
        "\n",
        "# 3) Define SeqDataset if not already defined (matches earlier signature)\n",
        "if 'SeqDataset' not in globals():\n",
        "    from torch.utils.data import Dataset\n",
        "    class SeqDataset(Dataset):\n",
        "        def __init__(self, df: pd.DataFrame, features, lookback=64):\n",
        "            self.features = features; self.lookback = lookback\n",
        "            self.X, self.y = [], []\n",
        "            for _, g in df.groupby('Ticker', sort=False):\n",
        "                g = g.sort_index()  # ensure time order\n",
        "                Xg = g[features].values.astype(np.float32)\n",
        "                yg = g['y'].values.astype(np.int64)\n",
        "                for i in range(lookback, len(g)):\n",
        "                    self.X.append(Xg[i-lookback:i])\n",
        "                    self.y.append(yg[i])\n",
        "            self.X = np.array(self.X, dtype=np.float32)\n",
        "            self.y = np.array(self.y, dtype=np.int64)\n",
        "        def __len__(self): return len(self.y)\n",
        "        def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# 4) Build datasets and show lengths\n",
        "features = PARAMS['features']; lb = PARAMS['lookback']\n",
        "train_ds = SeqDataset(train_df, features, lookback=lb)\n",
        "val_ds   = SeqDataset(val_df,   features, lookback=lb)\n",
        "test_ds  = SeqDataset(test_df,  features, lookback=lb)\n",
        "\n",
        "print(\"SeqDataset lengths (boundary-safe):\")\n",
        "print(\" train_ds:\", len(train_ds))\n",
        "print(\" val_ds:  \", len(val_ds))\n",
        "print(\" test_ds: \", len(test_ds))\n",
        "\n",
        "# --- Dataset NaN/Inf check ---\n",
        "import math\n",
        "def check_ds(ds, name):\n",
        "    bad = 0\n",
        "    for i in range(0, min(len(ds), 5000), max(1, len(ds)//5000)):  # sample up to 5k evenly\n",
        "        x, y = ds[i]\n",
        "        if not np.isfinite(x).all() or not np.isfinite(y):\n",
        "            bad += 1; break\n",
        "    print(f\"{name}: {'OK' if bad==0 else 'HAS NaN/Inf'}\")\n",
        "\n",
        "check_ds(train_ds, \"train_ds\")\n",
        "check_ds(val_ds,   \"val_ds\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY9lkldkOgQG",
        "outputId": "5ac7da4e-676c-4f1f-9b4f-bc29e0aec966"
      },
      "id": "BY9lkldkOgQG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  2406 tickers |   5036427 rows\n",
            "val:  2406 tickers |    606112 rows\n",
            "test:  2406 tickers |   1778231 rows\n",
            "train:  2406 tickers |   5034021 rows (boundary-safe)\n",
            "val:  2406 tickers |    603706 rows (boundary-safe)\n",
            "test:  2406 tickers |   1718649 rows (boundary-safe)\n",
            "Post-normalization sanity (should be ~0/1 on train):\n",
            " train means:\n",
            " pct_change     -0.0\n",
            "roll_mean_10    0.0\n",
            "roll_vol_10     0.0\n",
            "rsi_14          0.0\n",
            "dtype: float64\n",
            " train stds:\n",
            " pct_change      1.0\n",
            "roll_mean_10    1.0\n",
            "roll_vol_10     1.0\n",
            "rsi_14          1.0\n",
            "dtype: float64\n",
            "SeqDataset lengths (boundary-safe):\n",
            " train_ds: 4880037\n",
            " val_ds:   449722\n",
            " test_ds:  1564665\n",
            "train_ds: OK\n",
            "val_ds: OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH = min(128, PARAMS['batch_size'])  # safe to start\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print(\"Batches per epoch:\",\n",
        "      len(train_loader), \"(train),\",\n",
        "      len(val_loader), \"(val) at batch_size=\", BATCH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZWnaIDL3Ckm",
        "outputId": "43f18890-04a8-4cc2-8766-15fc333ca6ef"
      },
      "id": "oZWnaIDL3Ckm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches per epoch: 38126 (train), 3514 (val) at batch_size= 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Wih2_bMcZD"
      },
      "source": [
        "## Model (Option A/B/C stubs)"
      ],
      "id": "m_Wih2_bMcZD"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GXvo5I5OMcZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d164a1f6-86db-46a3-a63c-cba26854ba5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model stubs ready (A/B in PyTorch, C via sklearn).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ConvTransformer(nn.Module):\n",
        "    def __init__(self, in_feats, d_model=64, nhead=8, dim_ff=128, num_layers=2, dropout=0.1, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv1d(in_channels=in_feats, out_channels=d_model, kernel_size=3, padding=1)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, F)\n",
        "        x = x.transpose(1, 2)          # (B, F, T)\n",
        "        x = torch.relu(self.proj(x))   # (B, d_model, T)\n",
        "        x = x.transpose(1, 2)          # (B, T, d_model)\n",
        "        x = self.encoder(x)            # (B, T, d_model)\n",
        "        x = x.mean(dim=1)              # GAP over time\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNN_GRU(nn.Module):\n",
        "    \"\"\"Option B: 1D-CNN + small GRU head → logits(2).\"\"\"\n",
        "    def __init__(self, in_feats, hidden=32, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_feats, out_channels=hidden, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.gru = nn.GRU(input_size=hidden, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)       # (B, F, T)\n",
        "        x = self.conv(x)            # (B, H, T)\n",
        "        x = x.transpose(1, 2)       # (B, T, H)\n",
        "        _, h = self.gru(x)          # h: (1, B, H)\n",
        "        h = h.squeeze(0)\n",
        "        return self.fc(h)\n",
        "\n",
        "def build_model(option: str, in_feats: int):\n",
        "    if option == 'A':\n",
        "        return ConvTransformer(in_feats)\n",
        "    raise ValueError('Only Option A is enabled right now')\n",
        "\n",
        "print('Model stubs ready (A/B in PyTorch, C via sklearn).')\n"
      ],
      "id": "GXvo5I5OMcZE"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train (1-epoch smoke test) ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = build_model(PARAMS['model_option'], in_feats=len(PARAMS['features'])).to(device)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=PARAMS['lr'])\n",
        "\n",
        "# AMP only helps if GPU is available\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
        "\n",
        "def run_val(loader):\n",
        "    model.eval(); correct=total=0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred==yb).sum().item(); total += yb.numel()\n",
        "    return correct / max(total,1)\n",
        "\n",
        "model.train()\n",
        "for xb, yb in train_loader:\n",
        "    xb = xb.to(device); yb = yb.to(device)\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    if device == 'cuda':\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits = model(xb)\n",
        "            loss = crit(logits, yb)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "    else:\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    break  # one epoch “smoke” = one pass through first batch is enough to test plumbing\n",
        "\n",
        "val_acc = run_val(val_loader)\n",
        "print(f\"Smoke test complete. Val acc: {val_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCtDCAGe6Mhe",
        "outputId": "9db73b9b-bf30-4087-e023-79bc6623538f"
      },
      "id": "XCtDCAGe6Mhe",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-50293955.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
            "/tmp/ipython-input-50293955.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoke test complete. Val acc: 0.520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train: stabilized (no AMP), lower LR, grad clipping ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = build_model(PARAMS['model_option'], in_feats=len(PARAMS['features'])).to(device)\n",
        "crit  = nn.CrossEntropyLoss()\n",
        "opt   = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)  # lower LR + small WD\n",
        "\n",
        "def run_val(loader):\n",
        "    model.eval(); correct=total=0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            logits = model(xb)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred==yb).sum().item(); total += yb.numel()\n",
        "    return correct / max(total,1)\n",
        "\n",
        "best_acc = -1.0\n",
        "best_path = os.path.join(OUTPUT_DIR, 'best_model.pt')\n",
        "\n",
        "for ep in range(1, 3):  # start with 2 epochs to verify stability/speed\n",
        "    model.train()\n",
        "    running = 0.0; steps = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # grad clip\n",
        "        opt.step()\n",
        "        running += float(loss.item()); steps += 1\n",
        "        # (optional) speed probe:\n",
        "        # if steps % 5000 == 0: break\n",
        "\n",
        "    val_acc = run_val(val_loader)\n",
        "    print(f\"Epoch {ep}/2 | train_loss ~ {running/max(steps,1):.4f} | val_acc={val_acc:.3f}\")\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"  ↳ saved {best_path} (best so far)\")\n",
        "\n",
        "print(f\"Best val_acc: {best_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59jEmFp9F10F",
        "outputId": "43bdd874-879a-4275-e2fc-47c6be973782"
      },
      "id": "59jEmFp9F10F",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/2 | train_loss ~ 0.6795 | val_acc=0.535\n",
            "  ↳ saved ./outputs/best_model.pt (best so far)\n",
            "Epoch 2/2 | train_loss ~ 0.6777 | val_acc=0.540\n",
            "  ↳ saved ./outputs/best_model.pt (best so far)\n",
            "Best val_acc: 0.540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYapWNS5McZF"
      },
      "source": [
        "## Train (per split)"
      ],
      "id": "VYapWNS5McZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmJ-S01McZF"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "def train_torch(model, train_ds, val_ds, epochs, lr, batch_size, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # --- DataLoaders ---\n",
        "    BATCH = min(128, PARAMS['batch_size'])  # start safe; can raise later on T4\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True)\n",
        "    len(train_ds), len(val_ds), BATCH\n",
        "    best, best_path = -1, os.path.join(OUTPUT_DIR, 'best_model.pt')\n",
        "\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "        # val\n",
        "        model.eval(); correct=total=0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred = logits.argmax(1)\n",
        "                correct += (pred==yb).sum().item(); total += yb.numel()\n",
        "        acc = correct/max(total,1)\n",
        "        print(f'Epoch {ep+1}/{epochs} val_acc={acc:.3f}')\n",
        "        if acc > best:\n",
        "            best = acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    print('Best val_acc:', best)\n",
        "    return best_path\n",
        "\n",
        "def train_sklearn_gbdt(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Option C: simple GradientBoostingClassifier. TODO: Save to outputs/.\"\"\"\n",
        "    clf = GradientBoostingClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    print('GBDT val acc:', clf.score(X_val, y_val))\n",
        "    return clf\n",
        "\n",
        "print('Training stubs ready.')\n"
      ],
      "id": "3vmJ-S01McZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0yeX_KdMcZG"
      },
      "source": [
        "## Inference & backtest"
      ],
      "id": "Z0yeX_KdMcZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbaw_9fKMcZG"
      },
      "outputs": [],
      "source": [
        "def infer_proba_torch(model, ds, batch_size=256, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"Return class-1 probabilities for each sample in dataset.\"\"\"\n",
        "    model = model.to(device)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "    probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "            probs.append(p)\n",
        "    return np.concatenate(probs)\n",
        "\n",
        "def positions_from_probs(probs, th_long, th_short):\n",
        "    \"\"\"Map probabilities → positions: +1 (long) / 0 (flat) / -1 (short).\"\"\"\n",
        "    pos = np.zeros_like(probs, dtype=int)\n",
        "    pos[probs >= th_long] = 1\n",
        "    pos[probs <= th_short] = -1\n",
        "    return pos\n",
        "\n",
        "def backtest_from_positions(returns, positions, cost_bps=5):\n",
        "    \"\"\"\n",
        "    Compute PnL with costs per trade. Returns equity curve and trades DataFrame.\n",
        "    TODO: ensure alignment (positions at t apply to ret at t+1 if using next-day execution).\n",
        "    \"\"\"\n",
        "    positions = positions.astype(int)\n",
        "    # Shift positions to apply next day\n",
        "    pos_shift = np.roll(positions, 1)\n",
        "    pos_shift[0] = 0\n",
        "    # Trading costs on position changes\n",
        "    trades = (np.abs(pos_shift[1:] - pos_shift[:-1]) > 0).astype(int)\n",
        "    cost = trades * (cost_bps / 1e4)\n",
        "    rets = pos_shift * returns\n",
        "    rets_adj = rets.copy()\n",
        "    rets_adj[1:] -= cost  # cost applied when trade occurs\n",
        "    equity = (1 + rets_adj).cumprod()\n",
        "    return equity, trades\n",
        "\n",
        "print('Inference/backtest stubs ready.')\n"
      ],
      "id": "zbaw_9fKMcZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdOv-D3eMcZH"
      },
      "source": [
        "## Metrics & plots"
      ],
      "id": "rdOv-D3eMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_b-YQxeMcZH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(equity_curve, daily_rets):\n",
        "    T = len(equity_curve)\n",
        "    if T <= 1:\n",
        "        return {'CAGR': 0, 'MaxDD': 0, 'Sharpe': 0}\n",
        "    years = T / 252\n",
        "    cagr = (equity_curve[-1] ** (1/years)) - 1 if years > 0 else 0\n",
        "    peaks = np.maximum.accumulate(equity_curve)\n",
        "    dd = (equity_curve / peaks) - 1\n",
        "    maxdd = dd.min()\n",
        "    sharpe = (np.mean(daily_rets) / (np.std(daily_rets) + 1e-8)) * np.sqrt(252)\n",
        "    return {'CAGR': float(cagr), 'MaxDD': float(maxdd), 'Sharpe': float(sharpe)}\n",
        "\n",
        "def plot_equity(equity_curve, title='Equity Curve', path=os.path.join(OUTPUT_DIR, 'equity_curve.png')):\n",
        "    plt.figure()\n",
        "    plt.plot(equity_curve)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Equity')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "print('Metrics/plots stubs ready.')\n"
      ],
      "id": "x_b-YQxeMcZH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HEYApwMcZH"
      },
      "source": [
        "## Params dump & outputs"
      ],
      "id": "j6HEYApwMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPlXPOx5McZI"
      },
      "outputs": [],
      "source": [
        "def save_run(seed, params, metrics: dict):\n",
        "    run = {\n",
        "        'timestamp': dt.datetime.utcnow().isoformat() + 'Z',\n",
        "        'seed': seed,\n",
        "        'params': params,\n",
        "        'metrics': metrics,\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, 'run.json'), 'w') as f:\n",
        "        json.dump(run, f, indent=2)\n",
        "    print('Saved outputs/run.json')\n",
        "\n",
        "print('Output saving stub ready.')\n"
      ],
      "id": "fPlXPOx5McZI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHc4l_bjMcZI"
      },
      "source": [
        "---\n",
        "### TODO wiring plan (in this order)\n",
        "1. Load data for tickers with `load_all_tickers` using `PARAMS` dates.\n",
        "2. `get_splits` to obtain train/val/test DataFrames.\n",
        "3. Create `SeqDataset` for train/val/test (sequence length = `PARAMS['lookback']`).\n",
        "4. Build model via `build_model(PARAMS['model_option'], in_feats=len(PARAMS['features']))`.\n",
        "5. Train with `train_torch` (or `train_sklearn_gbdt` if Option C).\n",
        "6. Infer probabilities on **test** set with `infer_proba_torch`.\n",
        "7. Convert to positions with `positions_from_probs` using thresholds from `PARAMS`.\n",
        "8. Backtest with `backtest_from_positions` (costs in bps in `PARAMS`).\n",
        "9. Plot and save equity curve; export CSV of trades (TODO: implement export).\n",
        "10. Compute and save metrics; dump `run.json` and `params_seed.json` already created.\n",
        "\n",
        "**No extras. Ship the notebook.**\n"
      ],
      "id": "NHc4l_bjMcZI"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gsV6n4toMcY8",
        "uPueuQsHMcZA",
        "RwhtxRzKprt3",
        "6lra6mHfrkVu",
        "H3DBaQOssy_w"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}