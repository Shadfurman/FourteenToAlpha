{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadfurman/FourteenToAlpha/blob/main/trading_bot_scaffold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wILlIqQMcY2"
      },
      "source": [
        "# Trading Bot: Conv-Transformer → Backtest (Scaffold)\n",
        "\n",
        "End-to-end notebook scaffold. Fill in the TODOs only—no scope creep.\n",
        "\n",
        "**Sections:**\n",
        "- Setup & seed\n",
        "- Data & features\n",
        "- Split\n",
        "- Model\n",
        "- Train\n",
        "- Inference & backtest\n",
        "- Metrics & plots\n",
        "- Params dump & outputs\n"
      ],
      "id": "5wILlIqQMcY2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsV6n4toMcY8"
      },
      "source": [
        "## Setup & seed"
      ],
      "id": "gsV6n4toMcY8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Uihs1lMcY-",
        "outputId": "4fb4ba37-82df-43b5-987e-293c7f55b9c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed & PARAMS set; outputs/params_seed.json written.\n"
          ]
        }
      ],
      "source": [
        "# Minimal installs for Colab (comment any you don't need)\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip -q install yfinance pandas numpy scikit-learn matplotlib torch torchvision torchaudio --progress-bar off\n",
        "\n",
        "import os, json, math, random, time, datetime as dt\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # Option C fallback\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "SEED = 42  # <-- SINGLE SOURCE OF TRUTH\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)\n",
        "\n",
        "# Global paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "PARQUET_DIR = '/content/data_parquet'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# PARAMS — edit here only\n",
        "PARAMS = {\n",
        "    'tickers': ['SPY', 'QQQ', 'AAPL'],\n",
        "    'start': '2010-01-01',\n",
        "    'end': '2025-01-01',\n",
        "    'lookback': 64,  # window length for sequence models\n",
        "    'features': ['pct_change', 'roll_mean_10', 'roll_vol_10', 'rsi_14'],\n",
        "    'split': {\n",
        "        'train_start': '2010-01-01', 'train_end': '2018-12-31',\n",
        "        'val_start':   '2019-01-01', 'val_end':   '2019-12-31',\n",
        "        'test_start':  '2020-01-01', 'test_end':  '2024-12-31',\n",
        "    },\n",
        "    'model_option': 'A',  # 'A' (Conv+Transformer) | 'B' (CNN+GRU) | 'C' (GBDT)\n",
        "    'epochs': 12,\n",
        "    'batch_size': 256,\n",
        "    'lr': 1e-3,\n",
        "    'threshold_long': 0.55,\n",
        "    'threshold_short': 0.45,\n",
        "    'cost_bps': 5,\n",
        "}\n",
        "with open(os.path.join(OUTPUT_DIR, 'params_seed.json'), 'w') as f:\n",
        "    json.dump({'seed': SEED, 'params': PARAMS}, f, indent=2)\n",
        "print('Seed & PARAMS set; outputs/params_seed.json written.')\n"
      ],
      "id": "H9Uihs1lMcY-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPueuQsHMcZA"
      },
      "source": [
        "## Data & features"
      ],
      "id": "uPueuQsHMcZA"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8hKKGhFMcZB",
        "outputId": "00afde54-ee07-49ce-8584-308acf5cfa27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data/feature functions defined. Fill TODOs as needed.\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def load_ohlcv_yf(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"Load daily OHLCV from yfinance. TODO: cache to CSV in ./data.\"\"\"\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(f'No data for {ticker}')\n",
        "    df = df[['Open','High','Low','Close','Volume']].copy()\n",
        "    df.index.name = 'Date'\n",
        "    df['Ticker'] = ticker\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
        "    delta = series.diff()\n",
        "    up = delta.clip(lower=0)\n",
        "    down = -delta.clip(upper=0)\n",
        "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
        "    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
        "    rs = roll_up / (roll_down + 1e-8)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Compute pct-change, rolling mean/vol, RSI(14). Add next-day direction target.\"\"\"\n",
        "    out = df.copy()\n",
        "    out['pct_change'] = out['Close'].pct_change()\n",
        "    out['roll_mean_10'] = out['pct_change'].rolling(10).mean()\n",
        "    out['roll_vol_10'] = out['pct_change'].rolling(10).std()\n",
        "    out['rsi_14'] = compute_rsi(out['Close'], 14)\n",
        "    # Target: next-day direction (1 if up, 0 if down or flat)\n",
        "    out['ret_fwd_1'] = out['Close'].pct_change().shift(-1)\n",
        "    out['y'] = (out['ret_fwd_1'] > 0).astype(int)\n",
        "    out = out.dropna()\n",
        "    return out\n",
        "\n",
        "def load_all_tickers(tickers, start, end):\n",
        "    frames = []\n",
        "    for t in tickers:\n",
        "        raw = load_ohlcv_yf(t, start, end)\n",
        "        feats = make_features(raw)\n",
        "        frames.append(feats)\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "# TODO: Optionally visualize a sample\n",
        "print('Data/feature functions defined. Fill TODOs as needed.')\n"
      ],
      "id": "N8hKKGhFMcZB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV→Parquet converter"
      ],
      "metadata": {
        "id": "RwhtxRzKprt3"
      },
      "id": "RwhtxRzKprt3"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSV → Parquet (one-time) ---\n",
        "import glob\n",
        "\n",
        "# Your paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "PARQUET_DIR = '/content/data_parquet'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# Map your CSV headers to canonical names we use elsewhere\n",
        "CSV_COLS = {\n",
        "    'Date': 'Date',\n",
        "    'Open': 'Open',\n",
        "    'High': 'High',\n",
        "    'Low': 'Low',\n",
        "    'Close': 'Close',                 # raw close (we'll replace with Adjusted Close for features)\n",
        "    'Volume': 'Volume',\n",
        "    'Adjusted Close': 'AdjClose',\n",
        "}\n",
        "\n",
        "def guess_ticker_from_path(p):\n",
        "    import os\n",
        "    return os.path.splitext(os.path.basename(p))[0].upper()\n",
        "\n",
        "def load_csv_one(p):\n",
        "    # Read permissively (handles odd rows/column orders)\n",
        "    df = pd.read_csv(\n",
        "        p,\n",
        "        engine='python',           # robust parser\n",
        "        on_bad_lines='skip'        # skip corrupted rows\n",
        "    )\n",
        "\n",
        "    # Normalize column names (lower -> canonical)\n",
        "    orig_cols = df.columns.tolist()\n",
        "    lc_map = {c.lower().strip(): c for c in df.columns}\n",
        "\n",
        "    # Find variants\n",
        "    date_col = lc_map.get('date')\n",
        "    open_col = lc_map.get('open')\n",
        "    high_col = lc_map.get('high')\n",
        "    low_col  = lc_map.get('low')\n",
        "    close_col= lc_map.get('close')\n",
        "    vol_col  = lc_map.get('volume')\n",
        "    adj_col  = lc_map.get('adjusted close') or lc_map.get('adj close') or lc_map.get('adjclose')\n",
        "\n",
        "    # Hard fail if no Date\n",
        "    if not date_col:\n",
        "        raise ValueError(f'No Date column in {p} (had: {orig_cols})')\n",
        "\n",
        "    # Build a minimal frame with whatever we found\n",
        "    cols = {}\n",
        "    cols['Date']   = date_col\n",
        "    if open_col:  cols['Open']  = open_col\n",
        "    if high_col:  cols['High']  = high_col\n",
        "    if low_col:   cols['Low']   = low_col\n",
        "    if close_col: cols['Close'] = close_col\n",
        "    if vol_col:   cols['Volume']= vol_col\n",
        "    if adj_col:   cols['AdjClose'] = adj_col\n",
        "\n",
        "    df = df.rename(columns={v:k for k,v in cols.items()})[list(cols.keys())]\n",
        "\n",
        "    # Parse day-first dates like 15-12-2010\n",
        "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "    df = df.dropna(subset=['Date']).sort_values('Date').set_index('Date')\n",
        "\n",
        "    # Prefer adjusted close for features; fall back to Close\n",
        "    if 'AdjClose' in df.columns:\n",
        "        df['Close_raw'] = df.get('Close', df['AdjClose'])\n",
        "        df['Close'] = df['AdjClose']\n",
        "    else:\n",
        "        # No adjusted close available; use raw Close\n",
        "        if 'Close' not in df.columns:\n",
        "            raise ValueError(f'No Close/AdjClose in {p} (had: {orig_cols})')\n",
        "        df['Close_raw'] = df['Close']\n",
        "\n",
        "    # Coerce numerics (strip stray chars like $ or commas)\n",
        "    for c in ['Open','High','Low','Close','Volume','AdjClose','Close_raw']:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.replace(r'[^0-9\\.\\-eE]', '', regex=True)\n",
        "                           .replace({'': None}))\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "    # Basic sanity: drop rows missing Close or Volume\n",
        "    need = ['Close']\n",
        "    if 'Volume' in df.columns: need.append('Volume')\n",
        "    df = df.dropna(subset=need)\n",
        "\n",
        "    df['Ticker'] = guess_ticker_from_path(p)\n",
        "    return df\n",
        "\n",
        "def iter_csv_files():\n",
        "    for d in DATA_IN_DIRS:\n",
        "        for p in glob.glob(os.path.join(d, '*.csv')):\n",
        "            yield p\n",
        "\n",
        "def build_parquet_once():\n",
        "    converted, skipped = 0, []\n",
        "    for p in iter_csv_files():\n",
        "        tkr = guess_ticker_from_path(p)\n",
        "        outp = os.path.join(PARQUET_DIR, f'{tkr}.parquet')\n",
        "        if os.path.exists(outp):\n",
        "            continue\n",
        "        try:\n",
        "            df = load_csv_one(p)\n",
        "            feats = make_features(df)  # uses adjusted Close internally now\n",
        "            feats = feats.drop(columns=[c for c in ['AdjClose','Close_raw'] if c in feats.columns], errors='ignore')\n",
        "            feats.to_parquet(outp)\n",
        "            converted += 1\n",
        "            if converted % 50 == 0:\n",
        "                print(f'Converted {converted} tickers...')\n",
        "        except Exception as e:\n",
        "            skipped.append((tkr, str(e)))\n",
        "            if len(skipped) <= 5:\n",
        "                print(f'[WARN] Skipping {tkr}: {e}')\n",
        "    print(f'Parquet conversion complete. Total tickers converted: {converted}. Skipped: {len(skipped)}')\n",
        "    if skipped:\n",
        "        import json\n",
        "        with open(os.path.join(OUTPUT_DIR, 'skipped_files.json'), 'w') as f:\n",
        "            json.dump(skipped, f, indent=2)\n",
        "        print('Wrote outputs/skipped_files.json')\n",
        "\n",
        "build_parquet_once()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBt_58EIqn1U",
        "outputId": "2ee884e3-84b0-4089-edc7-708371f43e68"
      },
      "id": "iBt_58EIqn1U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50 tickers...\n",
            "Converted 100 tickers...\n",
            "Converted 150 tickers...\n",
            "Converted 200 tickers...\n",
            "Converted 250 tickers...\n",
            "Converted 300 tickers...\n",
            "Converted 350 tickers...\n",
            "Converted 400 tickers...\n",
            "Converted 450 tickers...\n",
            "Converted 500 tickers...\n",
            "Converted 550 tickers...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parquet Loader"
      ],
      "metadata": {
        "id": "6lra6mHfrkVu"
      },
      "id": "6lra6mHfrkVu"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "def load_all_parquet(parquet_dir=PARQUET_DIR):\n",
        "    frames = [pd.read_parquet(p) for p in glob.glob(os.path.join(parquet_dir, '*.parquet'))]\n",
        "    if not frames:\n",
        "        raise RuntimeError('No parquet files found. Run build_parquet_once() first.')\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "full_df = load_all_parquet()\n",
        "print('Rows:', len(full_df), 'Tickers:', full_df['Ticker'].nunique())\n",
        "full_df.head()"
      ],
      "metadata": {
        "id": "Mv0EewBFrq04"
      },
      "id": "Mv0EewBFrq04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYs4IQaU0ZEL",
        "outputId": "f2425197-e7ad-4e7f-c706-8dd5aeb76778"
      },
      "id": "eYs4IQaU0ZEL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgWbAE8HMcZC"
      },
      "source": [
        "## Split (rolling walk-forward)"
      ],
      "id": "LgWbAE8HMcZC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tFob2ZyMcZC"
      },
      "outputs": [],
      "source": [
        "def time_slice(df, start, end):\n",
        "    return df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "\n",
        "def get_splits(df, split_cfg):\n",
        "    \"\"\"\n",
        "    Returns train_df, val_df, test_df for the configured windows.\n",
        "    TODO: extend to multiple rolling folds if desired (not required for MVP).\n",
        "    \"\"\"\n",
        "    train_df = time_slice(df, split_cfg['train_start'], split_cfg['train_end'])\n",
        "    val_df   = time_slice(df, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "    test_df  = time_slice(df, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "print('Split functions ready.')\n"
      ],
      "id": "-tFob2ZyMcZC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Wih2_bMcZD"
      },
      "source": [
        "## Model (Option A/B/C stubs)"
      ],
      "id": "m_Wih2_bMcZD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXvo5I5OMcZE"
      },
      "outputs": [],
      "source": [
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, features, lookback=64):\n",
        "        self.features = features\n",
        "        self.lookback = lookback\n",
        "        # group by ticker to avoid crossing boundaries\n",
        "        self.groups = [g for _, g in df.groupby('Ticker')]\n",
        "        self.X, self.y = [], []\n",
        "        for g in self.groups:\n",
        "            Xg = g[features].values\n",
        "            yg = g['y'].values\n",
        "            for i in range(lookback, len(g)):\n",
        "                self.X.append(Xg[i-lookback:i])\n",
        "                self.y.append(yg[i])\n",
        "        self.X = np.array(self.X, dtype=np.float32)\n",
        "        self.y = np.array(self.y, dtype=np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class ConvTransformer(nn.Module):\n",
        "    def __init__(self, in_feats, d_model=64, nhead=8, dim_ff=128, num_layers=2, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels=in_feats, out_channels=d_model, kernel_size=3, padding=1)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)               # (B,F,T)\n",
        "        x = torch.relu(self.conv(x))        # (B,d_model,T)\n",
        "        x = x.transpose(1, 2)               # (B,T,d_model)\n",
        "        x = self.encoder(x)                 # (B,T,d_model)\n",
        "        x = x.mean(dim=1)                   # GAP over time\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNN_GRU(nn.Module):\n",
        "    \"\"\"Option B: 1D-CNN + small GRU head → logits(2).\"\"\"\n",
        "    def __init__(self, in_feats, hidden=32, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_feats, out_channels=hidden, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.gru = nn.GRU(input_size=hidden, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)       # (B, F, T)\n",
        "        x = self.conv(x)            # (B, H, T)\n",
        "        x = x.transpose(1, 2)       # (B, T, H)\n",
        "        _, h = self.gru(x)          # h: (1, B, H)\n",
        "        h = h.squeeze(0)\n",
        "        return self.fc(h)\n",
        "\n",
        "def build_model(option: str, in_feats: int):\n",
        "    if option == 'A':\n",
        "        return ConvTransformer(in_feats)\n",
        "    if option == 'B':\n",
        "        return CNN_GRU(in_feats)\n",
        "    if option == 'C':\n",
        "        return None  # handled via sklearn GradientBoostingClassifier\n",
        "    raise ValueError('Unknown model option')\n",
        "\n",
        "print('Model stubs ready (A/B in PyTorch, C via sklearn).')\n"
      ],
      "id": "GXvo5I5OMcZE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYapWNS5McZF"
      },
      "source": [
        "## Train (per split)"
      ],
      "id": "VYapWNS5McZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmJ-S01McZF"
      },
      "outputs": [],
      "source": [
        "def train_torch(model, train_ds, val_ds, epochs, lr, batch_size, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    best, best_path = -1, os.path.join(OUTPUT_DIR, 'best_model.pt')\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "        # val\n",
        "        model.eval(); correct=total=0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred = logits.argmax(1)\n",
        "                correct += (pred==yb).sum().item(); total += yb.numel()\n",
        "        acc = correct/max(total,1)\n",
        "        print(f'Epoch {ep+1}/{epochs} val_acc={acc:.3f}')\n",
        "        if acc > best:\n",
        "            best = acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    print('Best val_acc:', best)\n",
        "    return best_path\n",
        "\n",
        "def train_sklearn_gbdt(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Option C: simple GradientBoostingClassifier. TODO: Save to outputs/.\"\"\"\n",
        "    clf = GradientBoostingClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    print('GBDT val acc:', clf.score(X_val, y_val))\n",
        "    return clf\n",
        "\n",
        "print('Training stubs ready.')\n"
      ],
      "id": "3vmJ-S01McZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0yeX_KdMcZG"
      },
      "source": [
        "## Inference & backtest"
      ],
      "id": "Z0yeX_KdMcZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbaw_9fKMcZG"
      },
      "outputs": [],
      "source": [
        "def infer_proba_torch(model, ds, batch_size=256, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"Return class-1 probabilities for each sample in dataset.\"\"\"\n",
        "    model = model.to(device)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "    probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "            probs.append(p)\n",
        "    return np.concatenate(probs)\n",
        "\n",
        "def positions_from_probs(probs, th_long, th_short):\n",
        "    \"\"\"Map probabilities → positions: +1 (long) / 0 (flat) / -1 (short).\"\"\"\n",
        "    pos = np.zeros_like(probs, dtype=int)\n",
        "    pos[probs >= th_long] = 1\n",
        "    pos[probs <= th_short] = -1\n",
        "    return pos\n",
        "\n",
        "def backtest_from_positions(returns, positions, cost_bps=5):\n",
        "    \"\"\"\n",
        "    Compute PnL with costs per trade. Returns equity curve and trades DataFrame.\n",
        "    TODO: ensure alignment (positions at t apply to ret at t+1 if using next-day execution).\n",
        "    \"\"\"\n",
        "    positions = positions.astype(int)\n",
        "    # Shift positions to apply next day\n",
        "    pos_shift = np.roll(positions, 1)\n",
        "    pos_shift[0] = 0\n",
        "    # Trading costs on position changes\n",
        "    trades = (np.abs(pos_shift[1:] - pos_shift[:-1]) > 0).astype(int)\n",
        "    cost = trades * (cost_bps / 1e4)\n",
        "    rets = pos_shift * returns\n",
        "    rets_adj = rets.copy()\n",
        "    rets_adj[1:] -= cost  # cost applied when trade occurs\n",
        "    equity = (1 + rets_adj).cumprod()\n",
        "    return equity, trades\n",
        "\n",
        "print('Inference/backtest stubs ready.')\n"
      ],
      "id": "zbaw_9fKMcZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdOv-D3eMcZH"
      },
      "source": [
        "## Metrics & plots"
      ],
      "id": "rdOv-D3eMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_b-YQxeMcZH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(equity_curve, daily_rets):\n",
        "    T = len(equity_curve)\n",
        "    if T <= 1:\n",
        "        return {'CAGR': 0, 'MaxDD': 0, 'Sharpe': 0}\n",
        "    years = T / 252\n",
        "    cagr = (equity_curve[-1] ** (1/years)) - 1 if years > 0 else 0\n",
        "    peaks = np.maximum.accumulate(equity_curve)\n",
        "    dd = (equity_curve / peaks) - 1\n",
        "    maxdd = dd.min()\n",
        "    sharpe = (np.mean(daily_rets) / (np.std(daily_rets) + 1e-8)) * np.sqrt(252)\n",
        "    return {'CAGR': float(cagr), 'MaxDD': float(maxdd), 'Sharpe': float(sharpe)}\n",
        "\n",
        "def plot_equity(equity_curve, title='Equity Curve', path=os.path.join(OUTPUT_DIR, 'equity_curve.png')):\n",
        "    plt.figure()\n",
        "    plt.plot(equity_curve)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Equity')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "print('Metrics/plots stubs ready.')\n"
      ],
      "id": "x_b-YQxeMcZH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HEYApwMcZH"
      },
      "source": [
        "## Params dump & outputs"
      ],
      "id": "j6HEYApwMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPlXPOx5McZI"
      },
      "outputs": [],
      "source": [
        "def save_run(seed, params, metrics: dict):\n",
        "    run = {\n",
        "        'timestamp': dt.datetime.utcnow().isoformat() + 'Z',\n",
        "        'seed': seed,\n",
        "        'params': params,\n",
        "        'metrics': metrics,\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, 'run.json'), 'w') as f:\n",
        "        json.dump(run, f, indent=2)\n",
        "    print('Saved outputs/run.json')\n",
        "\n",
        "print('Output saving stub ready.')\n"
      ],
      "id": "fPlXPOx5McZI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHc4l_bjMcZI"
      },
      "source": [
        "---\n",
        "### TODO wiring plan (in this order)\n",
        "1. Load data for tickers with `load_all_tickers` using `PARAMS` dates.\n",
        "2. `get_splits` to obtain train/val/test DataFrames.\n",
        "3. Create `SeqDataset` for train/val/test (sequence length = `PARAMS['lookback']`).\n",
        "4. Build model via `build_model(PARAMS['model_option'], in_feats=len(PARAMS['features']))`.\n",
        "5. Train with `train_torch` (or `train_sklearn_gbdt` if Option C).\n",
        "6. Infer probabilities on **test** set with `infer_proba_torch`.\n",
        "7. Convert to positions with `positions_from_probs` using thresholds from `PARAMS`.\n",
        "8. Backtest with `backtest_from_positions` (costs in bps in `PARAMS`).\n",
        "9. Plot and save equity curve; export CSV of trades (TODO: implement export).\n",
        "10. Compute and save metrics; dump `run.json` and `params_seed.json` already created.\n",
        "\n",
        "**No extras. Ship the notebook.**\n"
      ],
      "id": "NHc4l_bjMcZI"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}