{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shadfurman/FourteenToAlpha/blob/main/trading_bot_scaffold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wILlIqQMcY2"
      },
      "source": [
        "# Trading Bot: Conv-Transformer → Backtest (Scaffold)\n",
        "\n",
        "End-to-end notebook scaffold. Fill in the TODOs only—no scope creep.\n",
        "\n",
        "**Sections:**\n",
        "- Setup & seed\n",
        "- Data & features\n",
        "- Split\n",
        "- Model\n",
        "- Train\n",
        "- Inference & backtest\n",
        "- Metrics & plots\n",
        "- Params dump & outputs\n"
      ],
      "id": "5wILlIqQMcY2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsV6n4toMcY8"
      },
      "source": [
        "## Setup & seed"
      ],
      "id": "gsV6n4toMcY8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9Uihs1lMcY-",
        "outputId": "6d59b890-41b7-4630-be79-5c49d91ba95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu [CPU]\n",
            "Seed & PARAMS set; outputs/params_seed.json written.\n"
          ]
        }
      ],
      "source": [
        "# Minimal installs for Colab (comment any you don't need)\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip -q install yfinance pandas numpy scikit-learn matplotlib torch torchvision torchaudio pyarrow --progress-bar off\n",
        "\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "SEED = 42  # <-- SINGLE SOURCE OF TRUTH\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_seed(SEED)\n",
        "\n",
        "# Global paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "PARQUET_DIR = '/content/data_parquet'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# PARAMS — edit here only\n",
        "PARAMS = {\n",
        "    'tickers': \"ALL_PARQUET\",  # will discover from parquet folder\n",
        "    'start': '1970-01-01',\n",
        "    'end': '2025-01-01',\n",
        "    'lookback': 64,\n",
        "    'features': ['pct_change', 'roll_mean_10', 'roll_vol_10', 'rsi_14'],\n",
        "    'split': {\n",
        "        'train_start': '2010-01-01', 'train_end': '2018-12-31',\n",
        "        'val_start':   '2019-01-01', 'val_end':   '2019-12-31',\n",
        "        'test_start':  '2020-01-01', 'test_end':  '2024-12-31',\n",
        "    },\n",
        "    'model_option': 'A',\n",
        "    'epochs': 12,\n",
        "    'batch_size': 256,\n",
        "    'lr': 1e-3,\n",
        "    'threshold_long': 0.55,\n",
        "    'threshold_short': 0.45,\n",
        "    'cost_bps': 5,\n",
        "}\n",
        "with open(os.path.join(OUTPUT_DIR, 'params_seed.json'), 'w') as f:\n",
        "    json.dump({'seed': SEED, 'params': PARAMS}, f, indent=2)\n",
        "\n",
        "# Quick environment check\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "gpu = torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\"\n",
        "print(f\"Device: {device} [{gpu}]\")\n",
        "print('Seed & PARAMS set; outputs/params_seed.json written.')\n"
      ],
      "id": "H9Uihs1lMcY-"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYs4IQaU0ZEL",
        "outputId": "afb7b27c-8772-40f6-99f8-63581dd58db9"
      },
      "id": "eYs4IQaU0ZEL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPueuQsHMcZA"
      },
      "source": [
        "## Data & features"
      ],
      "id": "uPueuQsHMcZA"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8hKKGhFMcZB",
        "outputId": "ea640645-d03e-4ee5-a2d4-222ec59861f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data/feature functions defined. Fill TODOs as needed.\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "\n",
        "def load_ohlcv_yf(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
        "    \"\"\"Load daily OHLCV from yfinance. TODO: cache to CSV in ./data.\"\"\"\n",
        "    df = yf.download(ticker, start=start, end=end, progress=False)\n",
        "    if df.empty:\n",
        "        raise ValueError(f'No data for {ticker}')\n",
        "    df = df[['Open','High','Low','Close','Volume']].copy()\n",
        "    df.index.name = 'Date'\n",
        "    df['Ticker'] = ticker\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series: pd.Series, period: int = 14) -> pd.Series:\n",
        "    series = series.astype(float)\n",
        "    delta = series.diff()\n",
        "    up = delta.clip(lower=0.0)\n",
        "    down = (-delta).clip(lower=0.0)\n",
        "    roll_up = up.ewm(alpha=1/period, adjust=False).mean()\n",
        "    roll_down = down.ewm(alpha=1/period, adjust=False).mean()\n",
        "    rs = roll_up / (roll_down + 1e-8)\n",
        "    return 100.0 - (100.0 / (1.0 + rs))\n",
        "\n",
        "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute pct-change, rolling mean/vol(10), RSI(14) on adjusted close if present.\n",
        "    Add next-day direction target y (1 if up, else 0). Drops NaNs and sorts index.\n",
        "    \"\"\"\n",
        "    out = df.copy()\n",
        "    out = out.sort_index()\n",
        "\n",
        "    # Prefer adjusted. If converter already replaced Close with adjusted, this is a no-op.\n",
        "    price = out['AdjClose'] if 'AdjClose' in out.columns else out['Close']\n",
        "\n",
        "    out['pct_change']   = price.pct_change()\n",
        "    out['roll_mean_10'] = out['pct_change'].rolling(window=10, min_periods=10).mean()\n",
        "    out['roll_vol_10']  = out['pct_change'].rolling(window=10, min_periods=10).std()\n",
        "    out['rsi_14']       = compute_rsi(price, 14)\n",
        "\n",
        "    # Target uses forward return on the same price series\n",
        "    out['ret_fwd_1'] = price.pct_change().shift(-1)\n",
        "    out['y'] = (out['ret_fwd_1'] > 0).astype(int)\n",
        "\n",
        "    # Final cleanup: drop rows with any NaNs in features/target\n",
        "    out = out.dropna(subset=['pct_change', 'roll_mean_10', 'roll_vol_10', 'rsi_14', 'ret_fwd_1', 'y'])\n",
        "    return out\n",
        "\n",
        "def load_all_tickers(tickers, start, end):\n",
        "    frames = []\n",
        "    for t in tickers:\n",
        "        raw = load_ohlcv_yf(t, start, end)\n",
        "        feats = make_features(raw)\n",
        "        frames.append(feats)\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "# TODO: Optionally visualize a sample\n",
        "print('Data/feature functions defined. Fill TODOs as needed.')\n"
      ],
      "id": "N8hKKGhFMcZB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV→Parquet converter"
      ],
      "metadata": {
        "id": "RwhtxRzKprt3"
      },
      "id": "RwhtxRzKprt3"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CSV → Parquet (one-time) ---\n",
        "import glob\n",
        "\n",
        "# Your paths\n",
        "OUTPUT_DIR = './outputs'\n",
        "DATA_IN_DIRS = [\n",
        "    '/content/drive/MyDrive/stock_market_data/nasdaq/csv',\n",
        "    '/content/drive/MyDrive/stock_market_data/nyse/csv',\n",
        "]\n",
        "PARQUET_DIR = '/content/data_parquet'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
        "\n",
        "# Map your CSV headers to canonical names we use elsewhere\n",
        "CSV_COLS = {\n",
        "    'Date': 'Date',\n",
        "    'Open': 'Open',\n",
        "    'High': 'High',\n",
        "    'Low': 'Low',\n",
        "    'Close': 'Close',                 # raw close (we'll replace with Adjusted Close for features)\n",
        "    'Volume': 'Volume',\n",
        "    'Adjusted Close': 'AdjClose',\n",
        "}\n",
        "\n",
        "def guess_ticker_from_path(p):\n",
        "    import os\n",
        "    return os.path.splitext(os.path.basename(p))[0].upper()\n",
        "\n",
        "def load_csv_one(p):\n",
        "    # Read permissively (handles odd rows/column orders)\n",
        "    df = pd.read_csv(\n",
        "        p,\n",
        "        engine='python',           # robust parser\n",
        "        on_bad_lines='skip'        # skip corrupted rows\n",
        "    )\n",
        "\n",
        "    # Normalize column names (lower -> canonical)\n",
        "    orig_cols = df.columns.tolist()\n",
        "    lc_map = {c.lower().strip(): c for c in df.columns}\n",
        "\n",
        "    # Find variants\n",
        "    date_col = lc_map.get('date')\n",
        "    open_col = lc_map.get('open')\n",
        "    high_col = lc_map.get('high')\n",
        "    low_col  = lc_map.get('low')\n",
        "    close_col= lc_map.get('close')\n",
        "    vol_col  = lc_map.get('volume')\n",
        "    adj_col  = lc_map.get('adjusted close') or lc_map.get('adj close') or lc_map.get('adjclose')\n",
        "\n",
        "    # Hard fail if no Date\n",
        "    if not date_col:\n",
        "        raise ValueError(f'No Date column in {p} (had: {orig_cols})')\n",
        "\n",
        "    # Build a minimal frame with whatever we found\n",
        "    cols = {}\n",
        "    cols['Date']   = date_col\n",
        "    if open_col:  cols['Open']  = open_col\n",
        "    if high_col:  cols['High']  = high_col\n",
        "    if low_col:   cols['Low']   = low_col\n",
        "    if close_col: cols['Close'] = close_col\n",
        "    if vol_col:   cols['Volume']= vol_col\n",
        "    if adj_col:   cols['AdjClose'] = adj_col\n",
        "\n",
        "    df = df.rename(columns={v:k for k,v in cols.items()})[list(cols.keys())]\n",
        "\n",
        "    # Parse day-first dates like 15-12-2010\n",
        "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
        "    df = df.dropna(subset=['Date']).sort_values('Date').set_index('Date')\n",
        "\n",
        "    # Prefer adjusted close for features; fall back to Close\n",
        "    if 'AdjClose' in df.columns:\n",
        "        df['Close_raw'] = df.get('Close', df['AdjClose'])\n",
        "        df['Close'] = df['AdjClose']\n",
        "    else:\n",
        "        # No adjusted close available; use raw Close\n",
        "        if 'Close' not in df.columns:\n",
        "            raise ValueError(f'No Close/AdjClose in {p} (had: {orig_cols})')\n",
        "        df['Close_raw'] = df['Close']\n",
        "\n",
        "    # Coerce numerics (strip stray chars like $ or commas)\n",
        "    for c in ['Open','High','Low','Close','Volume','AdjClose','Close_raw']:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.replace(r'[^0-9\\.\\-eE]', '', regex=True)\n",
        "                           .replace({'': None}))\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "\n",
        "    # Basic sanity: drop rows missing Close or Volume\n",
        "    need = ['Close']\n",
        "    if 'Volume' in df.columns: need.append('Volume')\n",
        "    df = df.dropna(subset=need)\n",
        "\n",
        "    df['Ticker'] = guess_ticker_from_path(p)\n",
        "    return df\n",
        "\n",
        "def iter_csv_files():\n",
        "    for d in DATA_IN_DIRS:\n",
        "        for p in glob.glob(os.path.join(d, '*.csv')):\n",
        "            yield p\n",
        "\n",
        "def build_parquet_once():\n",
        "    converted, skipped = 0, []\n",
        "    for p in iter_csv_files():\n",
        "        tkr = guess_ticker_from_path(p)\n",
        "        outp = os.path.join(PARQUET_DIR, f'{tkr}.parquet')\n",
        "        if os.path.exists(outp):\n",
        "            continue\n",
        "        try:\n",
        "            df = load_csv_one(p)\n",
        "            feats = make_features(df)  # uses adjusted Close internally now\n",
        "            feats = feats.drop(columns=[c for c in ['AdjClose','Close_raw'] if c in feats.columns], errors='ignore')\n",
        "            feats.to_parquet(outp)\n",
        "            converted += 1\n",
        "            if converted % 50 == 0:\n",
        "                print(f'Converted {converted} tickers...')\n",
        "        except Exception as e:\n",
        "            skipped.append((tkr, str(e)))\n",
        "            if len(skipped) <= 5:\n",
        "                print(f'[WARN] Skipping {tkr}: {e}')\n",
        "    print(f'Parquet conversion complete. Total tickers converted: {converted}. Skipped: {len(skipped)}')\n",
        "    if skipped:\n",
        "        import json\n",
        "        with open(os.path.join(OUTPUT_DIR, 'skipped_files.json'), 'w') as f:\n",
        "            json.dump(skipped, f, indent=2)\n",
        "        print('Wrote outputs/skipped_files.json')\n",
        "\n",
        "build_parquet_once()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBt_58EIqn1U",
        "outputId": "4716ebed-bb0a-4c76-f4d3-c7f2ab18e502"
      },
      "id": "iBt_58EIqn1U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50 tickers...\n",
            "Converted 100 tickers...\n",
            "Converted 150 tickers...\n",
            "Converted 200 tickers...\n",
            "Converted 250 tickers...\n",
            "Converted 300 tickers...\n",
            "Converted 350 tickers...\n",
            "Converted 400 tickers...\n",
            "Converted 450 tickers...\n",
            "Converted 500 tickers...\n",
            "Converted 550 tickers...\n",
            "Converted 600 tickers...\n",
            "Converted 650 tickers...\n",
            "Converted 700 tickers...\n",
            "Converted 750 tickers...\n",
            "Converted 800 tickers...\n",
            "Converted 850 tickers...\n",
            "Converted 900 tickers...\n",
            "Converted 950 tickers...\n",
            "Converted 1000 tickers...\n",
            "Converted 1050 tickers...\n",
            "Converted 1100 tickers...\n",
            "Converted 1150 tickers...\n",
            "Converted 1200 tickers...\n",
            "Converted 1250 tickers...\n",
            "Converted 1300 tickers...\n",
            "Converted 1350 tickers...\n",
            "Converted 1400 tickers...\n",
            "Converted 1450 tickers...\n",
            "Converted 1500 tickers...\n",
            "Converted 1550 tickers...\n",
            "Converted 1600 tickers...\n",
            "Converted 1650 tickers...\n",
            "Converted 1700 tickers...\n",
            "Converted 1750 tickers...\n",
            "Converted 1800 tickers...\n",
            "Converted 1850 tickers...\n",
            "Converted 1900 tickers...\n",
            "Converted 1950 tickers...\n",
            "Converted 2000 tickers...\n",
            "Converted 2050 tickers...\n",
            "Converted 2100 tickers...\n",
            "Converted 2150 tickers...\n",
            "Converted 2200 tickers...\n",
            "Converted 2250 tickers...\n",
            "Converted 2300 tickers...\n",
            "Converted 2350 tickers...\n",
            "Converted 2400 tickers...\n",
            "Converted 2450 tickers...\n",
            "Converted 2500 tickers...\n",
            "Converted 2550 tickers...\n",
            "Converted 2600 tickers...\n",
            "Converted 2650 tickers...\n",
            "Parquet conversion complete. Total tickers converted: 2664. Skipped: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parquet Loader"
      ],
      "metadata": {
        "id": "6lra6mHfrkVu"
      },
      "id": "6lra6mHfrkVu"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parquet path index (no big concat) ---\n",
        "import glob, os\n",
        "from datetime import datetime\n",
        "\n",
        "PARQUET_DIR = '/content/data_parquet'  # already set\n",
        "paths = sorted(glob.glob(os.path.join(PARQUET_DIR, '*.parquet')))\n",
        "\n",
        "# Utility to count rows in a date window for one ticker without keeping it in RAM\n",
        "def count_rows_in_range(p, start, end):\n",
        "    df = pd.read_parquet(p, columns=['Ticker'])  # small read\n",
        "    # quick read of index range by reloading with date filter\n",
        "    df = pd.read_parquet(p)  # still per-ticker, small-ish\n",
        "    df = df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    return len(df)\n",
        "\n",
        "split_cfg = PARAMS['split']\n",
        "# Build file lists for each split (only tickers with enough rows in ALL splits)\n",
        "keep_paths = []\n",
        "train_paths, val_paths, test_paths = [], [], []\n",
        "\n",
        "lookback = PARAMS['lookback']\n",
        "for p in paths:\n",
        "    n_train = count_rows_in_range(p, split_cfg['train_start'], split_cfg['train_end'])\n",
        "    n_val   = count_rows_in_range(p, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "    n_test  = count_rows_in_range(p, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "    if (n_train >= lookback + 2) and (n_val >= lookback + 2) and (n_test >= lookback + 2):\n",
        "        keep_paths.append(p)\n",
        "\n",
        "def filter_paths_by_range(paths, start, end):\n",
        "    out = []\n",
        "    for p in paths:\n",
        "        # quick check again to avoid reusing tiny tickers\n",
        "        n = count_rows_in_range(p, start, end)\n",
        "        if n >= lookback + 2:\n",
        "            out.append(p)\n",
        "    return out\n",
        "\n",
        "train_paths = filter_paths_by_range(keep_paths, split_cfg['train_start'], split_cfg['train_end'])\n",
        "val_paths   = filter_paths_by_range(keep_paths, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "test_paths  = filter_paths_by_range(keep_paths, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "\n",
        "print(f\"tickers kept: {len(keep_paths)}\")\n",
        "print(f\"train_paths: {len(train_paths)}, val_paths: {len(val_paths)}, test_paths: {len(test_paths)}\")"
      ],
      "metadata": {
        "id": "Mv0EewBFrq04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bca3743-6b57-4a4b-bff2-707aff5a28fe"
      },
      "id": "Mv0EewBFrq04",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tickers kept: 2406\n",
            "train_paths: 2406, val_paths: 2406, test_paths: 2406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build full_df for split"
      ],
      "metadata": {
        "id": "H3DBaQOssy_w"
      },
      "id": "H3DBaQOssy_w"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Build full_df for split (features + target only) ---\n",
        "import glob, os\n",
        "\n",
        "MIN_COLS = PARAMS['features'] + ['y', 'Ticker']\n",
        "\n",
        "def load_all_parquet_min(parquet_dir=PARQUET_DIR, columns=MIN_COLS):\n",
        "    frames = []\n",
        "    for p in glob.glob(os.path.join(parquet_dir, '*.parquet')):\n",
        "        frames.append(pd.read_parquet(p, columns=columns))\n",
        "    return pd.concat(frames).sort_index()\n",
        "\n",
        "full_df = load_all_parquet_min()\n",
        "print('full_df:', len(full_df), 'rows |', full_df['Ticker'].nunique(), 'tickers')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQvgb7Pys4ej",
        "outputId": "a2ffff40-299b-4b6f-9df2-0a8daf2f5282"
      },
      "id": "DQvgb7Pys4ej",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "full_df: 15236147 rows | 2660 tickers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgWbAE8HMcZC"
      },
      "source": [
        "## Split (rolling walk-forward)"
      ],
      "id": "LgWbAE8HMcZC"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-tFob2ZyMcZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3aefb5c-ad25-4e68-c86d-d8d05314247b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  2406 tickers |   5036427 rows\n",
            "val:  2406 tickers |    606112 rows\n",
            "test:  2406 tickers |   1778231 rows\n"
          ]
        }
      ],
      "source": [
        "# --- Walk-forward split + filter short tickers ---\n",
        "# assumes: full_df, PARAMS, get_splits() already defined\n",
        "\n",
        "def time_slice(df, start, end):\n",
        "    return df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "\n",
        "def get_splits(df, split_cfg):\n",
        "    train_df = time_slice(df, split_cfg['train_start'], split_cfg['train_end'])\n",
        "    val_df   = time_slice(df, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "    test_df  = time_slice(df, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "train_df, val_df, test_df = get_splits(full_df, PARAMS['split'])\n",
        "\n",
        "def min_len_ok(df, lookback):\n",
        "    # need at least lookback + 2 rows to form a label and 1-step shift\n",
        "    return df.groupby('Ticker').size() >= (lookback + 2)\n",
        "\n",
        "lookback = PARAMS['lookback']\n",
        "\n",
        "ok_train = set(min_len_ok(train_df, lookback).pipe(lambda s: s[s].index))\n",
        "ok_val   = set(min_len_ok(val_df,   lookback).pipe(lambda s: s[s].index))\n",
        "ok_test  = set(min_len_ok(test_df,  lookback).pipe(lambda s: s[s].index))\n",
        "\n",
        "# keep tickers present (and long enough) in ALL splits to avoid leakage/imbalance\n",
        "keep = ok_train & ok_val & ok_test\n",
        "\n",
        "def keep_tickers(df, tickers):\n",
        "    return df[df['Ticker'].isin(tickers)].copy()\n",
        "\n",
        "train_df = keep_tickers(train_df, keep)\n",
        "val_df   = keep_tickers(val_df,   keep)\n",
        "test_df  = keep_tickers(test_df,  keep)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"{name}: {df['Ticker'].nunique():>5} tickers | {len(df):>9} rows\")\n"
      ],
      "id": "-tFob2ZyMcZC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Walk-forward split + filter short tickers + build SeqDatasets\n"
      ],
      "metadata": {
        "id": "Zx-pG2pbOQYK"
      },
      "id": "Zx-pG2pbOQYK"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Walk-forward split + filter short tickers + build SeqDatasets (self-contained) ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1) Define get_splits if not already defined\n",
        "if 'get_splits' not in globals():\n",
        "    def time_slice(df, start, end):\n",
        "        return df.loc[(df.index >= pd.to_datetime(start)) & (df.index <= pd.to_datetime(end))]\n",
        "    def get_splits(df, split_cfg):\n",
        "        train_df = time_slice(df, split_cfg['train_start'], split_cfg['train_end'])\n",
        "        val_df   = time_slice(df, split_cfg['val_start'],   split_cfg['val_end'])\n",
        "        test_df  = time_slice(df, split_cfg['test_start'],  split_cfg['test_end'])\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "train_df, val_df, test_df = get_splits(full_df, PARAMS['split'])\n",
        "\n",
        "# 2) Keep only tickers with enough rows in ALL splits (avoid short series)\n",
        "def min_len_ok(df, lookback):\n",
        "    return df.groupby('Ticker').size() >= (lookback + 2)\n",
        "\n",
        "lookback = PARAMS['lookback']\n",
        "ok_train = set(min_len_ok(train_df, lookback).pipe(lambda s: s[s].index))\n",
        "ok_val   = set(min_len_ok(val_df,   lookback).pipe(lambda s: s[s].index))\n",
        "ok_test  = set(min_len_ok(test_df,  lookback).pipe(lambda s: s[s].index))\n",
        "keep = ok_train & ok_val & ok_test\n",
        "\n",
        "def keep_tickers(df, tickers):\n",
        "    return df[df['Ticker'].isin(tickers)].copy()\n",
        "\n",
        "train_df = keep_tickers(train_df, keep)\n",
        "val_df   = keep_tickers(val_df,   keep)\n",
        "test_df  = keep_tickers(test_df,  keep)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"{name}: {df['Ticker'].nunique():>5} tickers | {len(df):>9} rows\")\n",
        "\n",
        "# --- Fix boundary leakage: drop last row per ticker within each split ---\n",
        "def drop_last_per_ticker(df):\n",
        "    df = df.sort_index()\n",
        "    last_idx = df.groupby('Ticker').tail(1).index\n",
        "    return df.drop(index=last_idx)\n",
        "\n",
        "train_df = drop_last_per_ticker(train_df)\n",
        "val_df   = drop_last_per_ticker(val_df)\n",
        "test_df  = drop_last_per_ticker(test_df)\n",
        "\n",
        "for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    print(f\"{name}: {df['Ticker'].nunique():>5} tickers | {len(df):>9} rows (boundary-safe)\")\n",
        "\n",
        "# 3) Define SeqDataset if not already defined (matches earlier signature)\n",
        "if 'SeqDataset' not in globals():\n",
        "    from torch.utils.data import Dataset\n",
        "    class SeqDataset(Dataset):\n",
        "        def __init__(self, df: pd.DataFrame, features, lookback=64):\n",
        "            self.features = features; self.lookback = lookback\n",
        "            self.X, self.y = [], []\n",
        "            for _, g in df.groupby('Ticker', sort=False):\n",
        "                g = g.sort_index()  # ensure time order\n",
        "                Xg = g[features].values.astype(np.float32)\n",
        "                yg = g['y'].values.astype(np.int64)\n",
        "                for i in range(lookback, len(g)):\n",
        "                    self.X.append(Xg[i-lookback:i])\n",
        "                    self.y.append(yg[i])\n",
        "            self.X = np.array(self.X, dtype=np.float32)\n",
        "            self.y = np.array(self.y, dtype=np.int64)\n",
        "        def __len__(self): return len(self.y)\n",
        "        def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "# 4) Build datasets and show lengths\n",
        "features = PARAMS['features']; lb = PARAMS['lookback']\n",
        "train_ds = SeqDataset(train_df, features, lookback=lb)\n",
        "val_ds   = SeqDataset(val_df,   features, lookback=lb)\n",
        "test_ds  = SeqDataset(test_df,  features, lookback=lb)\n",
        "\n",
        "print(\"SeqDataset lengths (boundary-safe):\")\n",
        "print(\" train_ds:\", len(train_ds))\n",
        "print(\" val_ds:  \", len(val_ds))\n",
        "print(\" test_ds: \", len(test_ds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY9lkldkOgQG",
        "outputId": "10bad94a-9f00-4466-9653-8b9f057d86f1"
      },
      "id": "BY9lkldkOgQG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train:  2406 tickers |   5036427 rows\n",
            "val:  2406 tickers |    606112 rows\n",
            "test:  2406 tickers |   1778231 rows\n",
            "train:  2406 tickers |   5034021 rows (boundary-safe)\n",
            "val:  2406 tickers |    603706 rows (boundary-safe)\n",
            "test:  2406 tickers |   1718649 rows (boundary-safe)\n",
            "SeqDataset lengths (boundary-safe):\n",
            " train_ds: 4880037\n",
            " val_ds:   449722\n",
            " test_ds:  1564665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Wih2_bMcZD"
      },
      "source": [
        "## Model (Option A/B/C stubs)"
      ],
      "id": "m_Wih2_bMcZD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXvo5I5OMcZE"
      },
      "outputs": [],
      "source": [
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, features, lookback=64):\n",
        "        self.features = features\n",
        "        self.lookback = lookback\n",
        "        # group by ticker to avoid crossing boundaries\n",
        "        self.groups = [g for _, g in df.groupby('Ticker')]\n",
        "        self.X, self.y = [], []\n",
        "        for g in self.groups:\n",
        "            Xg = g[features].values\n",
        "            yg = g['y'].values\n",
        "            for i in range(lookback, len(g)):\n",
        "                self.X.append(Xg[i-lookback:i])\n",
        "                self.y.append(yg[i])\n",
        "        self.X = np.array(self.X, dtype=np.float32)\n",
        "        self.y = np.array(self.y, dtype=np.int64)\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class ConvTransformer(nn.Module):\n",
        "    def __init__(self, in_feats, d_model=64, nhead=8, dim_ff=128, num_layers=2, num_classes=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels=in_feats, out_channels=d_model, kernel_size=3, padding=1)\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)               # (B,F,T)\n",
        "        x = torch.relu(self.conv(x))        # (B,d_model,T)\n",
        "        x = x.transpose(1, 2)               # (B,T,d_model)\n",
        "        x = self.encoder(x)                 # (B,T,d_model)\n",
        "        x = x.mean(dim=1)                   # GAP over time\n",
        "        return self.fc(x)\n",
        "\n",
        "class CNN_GRU(nn.Module):\n",
        "    \"\"\"Option B: 1D-CNN + small GRU head → logits(2).\"\"\"\n",
        "    def __init__(self, in_feats, hidden=32, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=in_feats, out_channels=hidden, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.gru = nn.GRU(input_size=hidden, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)       # (B, F, T)\n",
        "        x = self.conv(x)            # (B, H, T)\n",
        "        x = x.transpose(1, 2)       # (B, T, H)\n",
        "        _, h = self.gru(x)          # h: (1, B, H)\n",
        "        h = h.squeeze(0)\n",
        "        return self.fc(h)\n",
        "\n",
        "def build_model(option: str, in_feats: int):\n",
        "    if option == 'A':\n",
        "        return ConvTransformer(in_feats)\n",
        "    if option == 'B':\n",
        "        return CNN_GRU(in_feats)\n",
        "    if option == 'C':\n",
        "        return None  # handled via sklearn GradientBoostingClassifier\n",
        "    raise ValueError('Unknown model option')\n",
        "\n",
        "print('Model stubs ready (A/B in PyTorch, C via sklearn).')\n"
      ],
      "id": "GXvo5I5OMcZE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYapWNS5McZF"
      },
      "source": [
        "## Train (per split)"
      ],
      "id": "VYapWNS5McZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vmJ-S01McZF"
      },
      "outputs": [],
      "source": [
        "def train_torch(model, train_ds, val_ds, epochs, lr, batch_size, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model = model.to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    best, best_path = -1, os.path.join(OUTPUT_DIR, 'best_model.pt')\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                logits = model(xb)\n",
        "                loss = crit(logits, yb)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "\n",
        "        # val\n",
        "        model.eval(); correct=total=0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits = model(xb)\n",
        "                pred = logits.argmax(1)\n",
        "                correct += (pred==yb).sum().item(); total += yb.numel()\n",
        "        acc = correct/max(total,1)\n",
        "        print(f'Epoch {ep+1}/{epochs} val_acc={acc:.3f}')\n",
        "        if acc > best:\n",
        "            best = acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    print('Best val_acc:', best)\n",
        "    return best_path\n",
        "\n",
        "def train_sklearn_gbdt(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Option C: simple GradientBoostingClassifier. TODO: Save to outputs/.\"\"\"\n",
        "    clf = GradientBoostingClassifier()\n",
        "    clf.fit(X_train, y_train)\n",
        "    print('GBDT val acc:', clf.score(X_val, y_val))\n",
        "    return clf\n",
        "\n",
        "print('Training stubs ready.')\n"
      ],
      "id": "3vmJ-S01McZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0yeX_KdMcZG"
      },
      "source": [
        "## Inference & backtest"
      ],
      "id": "Z0yeX_KdMcZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbaw_9fKMcZG"
      },
      "outputs": [],
      "source": [
        "def infer_proba_torch(model, ds, batch_size=256, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    \"\"\"Return class-1 probabilities for each sample in dataset.\"\"\"\n",
        "    model = model.to(device)\n",
        "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "    probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for xb, _ in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb)\n",
        "            p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
        "            probs.append(p)\n",
        "    return np.concatenate(probs)\n",
        "\n",
        "def positions_from_probs(probs, th_long, th_short):\n",
        "    \"\"\"Map probabilities → positions: +1 (long) / 0 (flat) / -1 (short).\"\"\"\n",
        "    pos = np.zeros_like(probs, dtype=int)\n",
        "    pos[probs >= th_long] = 1\n",
        "    pos[probs <= th_short] = -1\n",
        "    return pos\n",
        "\n",
        "def backtest_from_positions(returns, positions, cost_bps=5):\n",
        "    \"\"\"\n",
        "    Compute PnL with costs per trade. Returns equity curve and trades DataFrame.\n",
        "    TODO: ensure alignment (positions at t apply to ret at t+1 if using next-day execution).\n",
        "    \"\"\"\n",
        "    positions = positions.astype(int)\n",
        "    # Shift positions to apply next day\n",
        "    pos_shift = np.roll(positions, 1)\n",
        "    pos_shift[0] = 0\n",
        "    # Trading costs on position changes\n",
        "    trades = (np.abs(pos_shift[1:] - pos_shift[:-1]) > 0).astype(int)\n",
        "    cost = trades * (cost_bps / 1e4)\n",
        "    rets = pos_shift * returns\n",
        "    rets_adj = rets.copy()\n",
        "    rets_adj[1:] -= cost  # cost applied when trade occurs\n",
        "    equity = (1 + rets_adj).cumprod()\n",
        "    return equity, trades\n",
        "\n",
        "print('Inference/backtest stubs ready.')\n"
      ],
      "id": "zbaw_9fKMcZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdOv-D3eMcZH"
      },
      "source": [
        "## Metrics & plots"
      ],
      "id": "rdOv-D3eMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_b-YQxeMcZH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(equity_curve, daily_rets):\n",
        "    T = len(equity_curve)\n",
        "    if T <= 1:\n",
        "        return {'CAGR': 0, 'MaxDD': 0, 'Sharpe': 0}\n",
        "    years = T / 252\n",
        "    cagr = (equity_curve[-1] ** (1/years)) - 1 if years > 0 else 0\n",
        "    peaks = np.maximum.accumulate(equity_curve)\n",
        "    dd = (equity_curve / peaks) - 1\n",
        "    maxdd = dd.min()\n",
        "    sharpe = (np.mean(daily_rets) / (np.std(daily_rets) + 1e-8)) * np.sqrt(252)\n",
        "    return {'CAGR': float(cagr), 'MaxDD': float(maxdd), 'Sharpe': float(sharpe)}\n",
        "\n",
        "def plot_equity(equity_curve, title='Equity Curve', path=os.path.join(OUTPUT_DIR, 'equity_curve.png')):\n",
        "    plt.figure()\n",
        "    plt.plot(equity_curve)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Days')\n",
        "    plt.ylabel('Equity')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    return path\n",
        "\n",
        "print('Metrics/plots stubs ready.')\n"
      ],
      "id": "x_b-YQxeMcZH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6HEYApwMcZH"
      },
      "source": [
        "## Params dump & outputs"
      ],
      "id": "j6HEYApwMcZH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPlXPOx5McZI"
      },
      "outputs": [],
      "source": [
        "def save_run(seed, params, metrics: dict):\n",
        "    run = {\n",
        "        'timestamp': dt.datetime.utcnow().isoformat() + 'Z',\n",
        "        'seed': seed,\n",
        "        'params': params,\n",
        "        'metrics': metrics,\n",
        "    }\n",
        "    with open(os.path.join(OUTPUT_DIR, 'run.json'), 'w') as f:\n",
        "        json.dump(run, f, indent=2)\n",
        "    print('Saved outputs/run.json')\n",
        "\n",
        "print('Output saving stub ready.')\n"
      ],
      "id": "fPlXPOx5McZI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHc4l_bjMcZI"
      },
      "source": [
        "---\n",
        "### TODO wiring plan (in this order)\n",
        "1. Load data for tickers with `load_all_tickers` using `PARAMS` dates.\n",
        "2. `get_splits` to obtain train/val/test DataFrames.\n",
        "3. Create `SeqDataset` for train/val/test (sequence length = `PARAMS['lookback']`).\n",
        "4. Build model via `build_model(PARAMS['model_option'], in_feats=len(PARAMS['features']))`.\n",
        "5. Train with `train_torch` (or `train_sklearn_gbdt` if Option C).\n",
        "6. Infer probabilities on **test** set with `infer_proba_torch`.\n",
        "7. Convert to positions with `positions_from_probs` using thresholds from `PARAMS`.\n",
        "8. Backtest with `backtest_from_positions` (costs in bps in `PARAMS`).\n",
        "9. Plot and save equity curve; export CSV of trades (TODO: implement export).\n",
        "10. Compute and save metrics; dump `run.json` and `params_seed.json` already created.\n",
        "\n",
        "**No extras. Ship the notebook.**\n"
      ],
      "id": "NHc4l_bjMcZI"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}